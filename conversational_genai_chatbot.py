# -*- coding: utf-8 -*-
"""Conversational_GenAI_Chatbot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LQ-GF5_ssr_Jjk2_YqIanGZcVZEytYrW
"""

#it is required to use an Gen-AI model so installing Google's gemini here
!pip install -q langchain_google_genai

#required a framework so installing langchain
!pip install -q langchain

#langchain community is used in version2 of langchain for extended support of langchain framework
!pip install -q langchain-community

!pip install -q langchain-core

#setting up the google api key
import getpass
import os

os.environ["GOOGLE_API_KEY"] = getpass.getpass("Enter your Google AI API key: ")

#loading the genai model
from langchain_google_genai import ChatGoogleGenerativeAI
llm = ChatGoogleGenerativeAI(
    model="gemini-pro",
    temperature=0,
    convert_system_message_to_human=True)
llm

#checking the conectivity for the google geminipro
llm.invoke("what is machine learning?").content

#in order to maintain the conversation history and memory(state of the conversation) we are importing few necessary libraries
from langchain_core.chat_history import BaseChatMessageHistory,InMemoryChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory

#creating the session id to store the session details
store = {}
def get_session_history(session_id:str)->BaseChatMessageHistory:
  if session_id not in store:
    store[session_id]=InMemoryChatMessageHistory()
  return store[session_id]

#configuring the session ID
config = {"configurable":{"session_id":"first_chat"}}

model_memory = RunnableWithMessageHistory(llm,get_session_history)

#all the data will be stored inside the first_chat and if anything required apart from this session , configure another session id
model_memory.invoke("Hello, my name is karthik?",config=config).content

model_memory.invoke("whats my name?",config=config).content

# to see the stored session details in the store variable
store

from langchain_core.prompts import ChatPromptTemplate,MessagesPlaceholder

prompt = ChatPromptTemplate.from_messages(
    [
        ("system","you are a helpful AI conversational chatbot, answer all the questions upto your knowledge, dont make things up."),
        MessagesPlaceholder(variable_name="chat_history"),

    ]
)

chain = prompt | llm

chain.invoke({"chat_history":["my name is karthik"]}).content

memory_model = RunnableWithMessageHistory(chain,get_session_history)

config = {"configurable":{"session_id":"second_chat"}}

model_memory.invoke("what is deep learning?",config=config).content

model_memory.invoke("what is 2+2?",config=config).content

model_memory.invoke("is the previous math question related to addition or substraction?",config=config).content

store

